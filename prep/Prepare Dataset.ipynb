{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_EMAIL = re.compile(\n",
    "    r\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*)@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\")\n",
    "RE_URL = re.compile(r\"((https?|ftp|smtp):\\/\\/)?(www.)?[a-z0-9]+\\.[a-z]+(\\/[a-zA-Z0-9#]+\\/?)*\", re.IGNORECASE)\n",
    "RE_RELAX_PHONE = re.compile(r'(\\(? ?[\\d]{2,3} ?\\)?.{,3}?){2,}')\n",
    "# Taken from:\n",
    "# http://www.cs.cmu.edu/~vitor/papers/sigFilePaper_finalversion.pdf\n",
    "# Line matches the regular expression \"^[\\s]*---*[\\s]*$\".\n",
    "RE_SEPARATOR = re.compile(r'^[\\s]*---*[\\s]*')\n",
    "RE_REPLY = re.compile(r'^\\>')\n",
    "RE_REPLY_PUNCT = re.compile('^[^A-Za-z0-9]{1,2}\\>')\n",
    "RE_TAB = re.compile(r'\\t')\n",
    "RE_WROTE = re.compile(r'\\bwr[oi]tes?:\\b', re.IGNORECASE)\n",
    "\n",
    "# Taken from:\n",
    "# http://www.cs.cmu.edu/~vitor/papers/sigFilePaper_finalversion.pdf\n",
    "# Line has a sequence of 10 or more special characters.\n",
    "RE_SPECIAL_CHARS = re.compile(('^[\\s]*([\\*]|#|[\\+]|[\\^]|-|[\\~]|[\\&]|[\\$]|_|[\\!]|'\n",
    "                               '[\\/]|[\\%]|[\\:]|[\\=]){10,}[\\s]*$'))\n",
    "\n",
    "keywords = [r\"thank\",r\"regards\",r\"wishes\", r\"^sent from\", r\"\\bBR\\b\", r\"sincerely\", r\"corporation\", r\"\\bcorp\\b\", r\"\\bLLC\\b\", r\"group\", r\"fax\", r\"e?mail\", r\"phone\", r\"www\"]\n",
    "RE_SIGNATURE_WORDS = re.compile(\"|\".join([f\"({w})\" for w in keywords]), re.IGNORECASE)\n",
    "\n",
    "# Taken from:\n",
    "# http://www.cs.cmu.edu/~vitor/papers/sigFilePaper_finalversion.pdf\n",
    "# Line contains a pattern like Vitor R. Carvalho or William W. Cohen.\n",
    "RE_NAME = re.compile(r'[A-Z][a-z]+\\s\\s?[A-Z][\\.]?\\s\\s?[A-Z][a-z]+')\n",
    "\n",
    "RE_HEADER_WORDS = re.compile(r'from|\\bto\\b|subject|[Cc]c|[Bb]cc:|[Ff]orwarded', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def punct_percent(line):\n",
    "    if len(line) == 0:\n",
    "        return 0\n",
    "    punct = [c for c in line if c in string.punctuation]\n",
    "    return len(punct) / len(line)\n",
    "\n",
    "\n",
    "def alphanum_percent(line):\n",
    "    if len(line) == 0:\n",
    "        return 0\n",
    "    punct = [c for c in line if c.isalnum()]\n",
    "    return len(punct) / len(line)\n",
    "\n",
    "def num_percent(line):\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        return 0\n",
    "    punct = [c for c in line if c.isdigit()]\n",
    "    return len(punct) / len(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_name(text):\n",
    "    import email\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    if \"@\" not in text:\n",
    "        return text\n",
    "    \n",
    "    name, mail = email.utils.parseaddr(text)\n",
    "    if name:\n",
    "        return name\n",
    "    \n",
    "    name, domain = text.split(\"@\")\n",
    "    name_parts = [x.strip() for x in re.split(r\"@|\\.|\\W|\\_\", name) if x.strip()]\n",
    "    return \" \".join(name_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import nltk\n",
    "tw = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_title(line):\n",
    "    clean = \"\".join([x for x in line])\n",
    "    tokens = tw.tokenize(clean)\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    titles = [t for t in tokens if t.istitle() and len(t) > 2]\n",
    "    return len(titles)/len(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_person(spacy_doc):\n",
    "    if not spacy_doc.text.strip():\n",
    "        return False\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for ent in spacy_doc.ents:\n",
    "        labels.append(ent.label_)\n",
    "        sizes.append(ent.end_char-ent.start_char)\n",
    "    ratio = sum(sizes)/len(spacy_doc.text)\n",
    "    return 'PERSON' in labels and ratio > 0.7 and not spacy_doc.text.startswith(\"<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_org(spacy_doc):\n",
    "    if not spacy_doc.text.strip():\n",
    "        return False\n",
    "    labels = []\n",
    "    sizes = []\n",
    "    for ent in spacy_doc.ents:\n",
    "        labels.append(ent.label_)\n",
    "        sizes.append(ent.end_char-ent.start_char)\n",
    "    ratio = sum(sizes)/len(spacy_doc.text)\n",
    "    return 'ORG' in labels and ratio > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity(row):\n",
    "    for x in ['blank', 'email', 'url', 'phone', 'person', 'org']:\n",
    "        if row[x] == 1:\n",
    "            return x\n",
    "    return \"no_entity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "        'blank': lambda doc: 1 if len(doc[0].strip()) == 0 else 0,\n",
    "        'email': lambda doc: 1 if RE_EMAIL.search(doc[0]) else 0,\n",
    "        'url': lambda doc: 1 if RE_URL.search(doc[0]) else 0,\n",
    "        'phone': lambda doc: 1 if RE_RELAX_PHONE.search(doc[0]) else 0,\n",
    "        'sigdelimiter': lambda doc: 1 if RE_SEPARATOR.match(doc[0]) else 0,\n",
    "        'special': lambda doc: 1 if RE_SPECIAL_CHARS.search(doc[0]) else 0,\n",
    "        'words': lambda doc: 1 if RE_SIGNATURE_WORDS.search(doc[0]) else 0,\n",
    "        'header': lambda doc: 1 if RE_HEADER_WORDS.search(doc[0]) else 0,\n",
    "        'name': lambda doc: 1 if RE_NAME.search(doc[0]) else 0,\n",
    "        'endquote': lambda doc: 1 if doc[0].endswith(\"\\\"\") else 0,\n",
    "        'tabs1': lambda doc: 1 if len(RE_TAB.findall(doc[0])) == 1 else 0,\n",
    "        'tabs2': lambda doc: 1 if len(RE_TAB.findall(doc[0])) == 2 else 0,\n",
    "        'tabs3': lambda doc: 1 if len(RE_TAB.findall(doc[0])) >= 3 else 0,\n",
    "        'punct20': lambda doc: 1 if punct_percent(doc[0]) >= 0.2 else 0,\n",
    "        'punct50': lambda doc: 1 if punct_percent(doc[0]) >= 0.5 else 0,\n",
    "        'punct90': lambda doc: 1 if punct_percent(doc[0]) >= 0.9 else 0,\n",
    "        'reply': lambda doc: 1 if RE_REPLY.match(doc[0]) else 0,\n",
    "        'startpunct': lambda doc: 1 if doc[0].startswith(tuple(p for p in string.punctuation)) else 0,\n",
    "        'firstchar': lambda doc: doc[0][0] if len(doc[0]) > 0 else \"\",\n",
    "        'replypunct': lambda doc: 1 if RE_REPLY_PUNCT.match(doc[0]) else 0,\n",
    "        'wrote': lambda doc: 1 if RE_WROTE.search(doc[0]) else 0,\n",
    "        'alphanum90': lambda doc: 1 if alphanum_percent(doc[0]) < 0.9 else 0, # i.e. more than 10% special symbols\n",
    "        'alphanum50': lambda doc: 1 if alphanum_percent(doc[0]) < 0.5 else 0,\n",
    "        'alphanum10': lambda doc: 1 if alphanum_percent(doc[0]) < 0.1 else 0,\n",
    "        'num90': lambda doc: 1 if num_percent(doc[0]) >= 0.9 else 0,  # i.e. numbers are 90% of the line\n",
    "        'num50': lambda doc: 1 if num_percent(doc[0]) >= 0.5 else 0,\n",
    "        'num10': lambda doc: 1 if num_percent(doc[0]) >= 0.1 else 0,\n",
    "        'title': lambda doc: 1 if doc[0].strip().istitle() else 0,\n",
    "        'many_titles': lambda doc: 1 if percent_title(doc[0]) >= 0.5 else 0,\n",
    "        'person': lambda doc: 1 if is_person(doc[1]) else 0,\n",
    "        'org': lambda doc: 1 if is_org(doc[1]) else 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_sender(line, sender):\n",
    "    low_line = line.lower()\n",
    "    name, sep, domain = sender.partition(\"@\")\n",
    "    full_name = \" \".join(name.split(\".\"))\n",
    "    return sender.lower() in low_line or full_name.lower() in low_line or name in low_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_PATTERN = \"^#sig#\"\n",
    "def line_to_entity(line, filename, i):\n",
    "    # sender = get_sender(filename)\n",
    "    m = re.match(ENTITY_PATTERN, line)\n",
    "    if m:\n",
    "        spacy_doc = nlp(line[5:])\n",
    "        e = {\n",
    "            \"line\": line[5:],\n",
    "            \"filename\": filename,\n",
    "            \"entity\": \"signature\",\n",
    "            \"len\": len(line[5:]),\n",
    "            \"lineNo\": i+1\n",
    "        }\n",
    "    else:\n",
    "        spacy_doc = nlp(line)\n",
    "        e = {\"line\": line, \"filename\": filename, \"entity\": \"no_entity\", \"len\": len(line), \"lineNo\": i+1}    \n",
    "    doc = (e[\"line\"], spacy_doc)\n",
    "    for feature, fn in feature_dict.items():            \n",
    "        e[feature] = fn(doc)    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blanks(lines):    \n",
    "    return [line for line in lines if len(line.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_blank_lines(lines, leading=True, trailing=True):\n",
    "    leading_blank = 0\n",
    "    trailing_blank = len(lines)\n",
    "    lines_it = iter(lines)\n",
    "    next_line = next(lines_it, None)\n",
    "    while next_line is not None and len(next_line.strip()) == 0:\n",
    "        leading_blank += 1\n",
    "        next_line = next(lines_it, None)\n",
    "\n",
    "    if trailing:\n",
    "        it_reversed = iter(reversed(lines))\n",
    "        next_line = next(it_reversed, None)\n",
    "        while next_line is not None and len(next_line.strip()) == 0:\n",
    "            trailing_blank -= 1\n",
    "            next_line = next(it_reversed, None)\n",
    "    return lines[leading_blank:trailing_blank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature_length(ents):\n",
    "    \"\"\" Signature length in number of lines \"\"\"\n",
    "    return sum(1 for e in ents if e[\"entity\"] == \"signature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it has multiple signatures\n",
    "def detect_signature_positions(lines):\n",
    "    \"\"\"Returns list of lists, each with lineNo of the signatures\"\"\"\n",
    "    sigs = [i for i, line in enumerate(lines) if re.match(ENTITY_PATTERN, line)]\n",
    "    sig_groups = []\n",
    "    cur_group = []\n",
    "    for n in sorted(sigs):\n",
    "        last_n = cur_group[-1] if cur_group else None\n",
    "        if not last_n:\n",
    "            cur_group.append(n)\n",
    "            continue\n",
    "        if n-last_n == 1:\n",
    "            cur_group.append(n)\n",
    "        else:\n",
    "            sig_groups.append(cur_group)\n",
    "            cur_group = []\n",
    "    if cur_group:\n",
    "        sig_groups.append(cur_group)\n",
    "    return sig_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sender(filename):\n",
    "    sender = senders[filename]\n",
    "    return sender if sender != \"None\" else \"None@enron.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filenames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-05abaa47611f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msenders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_sender\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msenders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filenames' is not defined"
     ]
    }
   ],
   "source": [
    "senders = {}\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(files_path, filename+\"_sender\"), mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        senders[filename] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = r\"F:\\Documents\\stopansko\\masters\\thesis\\sig-detect\\data\\clean\\enron_random_clean\"\n",
    "#files_path = r\"F:\\Documents\\stopansko\\masters\\thesis\\sig-detect\\data\\clean\\enron_random\"\n",
    "#files_path = r\"F:\\Documents\\stopansko\\masters\\thesis\\sig-detect\\data\\clean\\sigPlusReply\"\n",
    "filenames = [f for f in os.listdir(files_path) if os.path.isfile(os.path.join(files_path, f)) and \"_sender\" not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list()\n",
    "entities = list()\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(files_path, filename), mode=\"r\", encoding=\"utf-8\") as f:        \n",
    "        lines = f.read().splitlines()\n",
    "        lines = strip_blank_lines(lines)\n",
    "        blocks = detect_signature_positions(lines)\n",
    "        non_blanks = remove_blanks(lines)\n",
    "        if len(non_blanks) > 0:\n",
    "            lengths = [len(line) for line in lines]\n",
    "            file_entities = [line_to_entity(line, filename, i) for i, line in enumerate(lines)]\n",
    "            entities.extend(file_entities)\n",
    "            files.append({\n",
    "                \"filename\": filename,\n",
    "                \"nlines\": len(lines),\n",
    "                \"len_avg\": np.ceil(np.mean(lengths)),\n",
    "                \"len_min\": min(lengths),\n",
    "                \"len_max\": max(lengths),\n",
    "                \"nBlanks\": len(lines) - len(non_blanks),\n",
    "                \"nNonBlanks\": len(non_blanks),\n",
    "                \"nSig\": get_signature_length(file_entities), # lengh of the signature in lines\n",
    "                \"nSigBlocks\": len(blocks)\n",
    "            })\n",
    "        \n",
    "df_files = pd.DataFrame(files) \n",
    "entities = pd.DataFrame(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = entities.merge(df_files, on=\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "master[\"posFromEnd\"] = master.nlines - master.lineNo\n",
    "master[\"last\"] = master.posFromEnd.apply(lambda x: 1 if x == 0 else 0)\n",
    "master[\"prevlast\"] = master.posFromEnd.apply(lambda x: 1 if x == 1 else 0)\n",
    "master[\"last5\"] = master.posFromEnd.apply(lambda x: 1 if x < 5 else 0)\n",
    "master[\"last11\"] = master.posFromEnd.apply(lambda x: 1 if x < 11 else 0)\n",
    "master[\"posRatio\"] = master.lineNo / master.nlines # 1 = last\n",
    "master[\"posRatioFromEnd\"] = master.posFromEnd / master.nlines\n",
    "master[\"posRatioNB\"] = master.lineNo / master.nNonBlanks # 1 = last\n",
    "master[\"lenRatio\"] = master.len / master.len_avg\n",
    "master[\"lenRatioMax\"] = master.len / master.len_max\n",
    "\n",
    "master[\"less_avg_len\"] = master.apply(lambda row: 1 if row.len <= row.len_avg else 0, axis=1)\n",
    "master[\"more_avg_len\"] = master.apply(lambda row: 1 if row.len > row.len_avg else 0, axis=1)\n",
    "master[\"less_avg_len75\"] = master.apply(lambda row: 1 if row.len <= (row.len_avg*.75) else 0, axis=1)\n",
    "master[\"less_avg_len50\"] = master.apply(lambda row: 1 if row.len <= (row.len_avg*.5) else 0, axis=1)\n",
    "\n",
    "master[\"named_entity\"] = master.apply(lambda row: named_entity(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "master[\"pred_file\"] = master.filename.shift(1)\n",
    "master[\"next_file\"] = master.filename.shift(-1)\n",
    "master[\"pred_named_entity\"] = master.named_entity.shift(1)\n",
    "master[\"next_named_entity\"] = master.named_entity.shift(-1)\n",
    "master[\"prev_same_entity\"] = master.apply(lambda row: 1 if row.named_entity == row.pred_named_entity else 0, axis=1)\n",
    "master[\"next_same_entity\"] = master.apply(lambda row: 1 if row.named_entity == row.next_named_entity else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = master[master.entity==\"signature\"]\n",
    "negative = master[master.entity!=\"signature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "entity_order = ['no_entity', 'person', 'org', 'blank', 'email', 'url', 'phone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_entity = 47.61% of negatives\n",
      "person = 2.75% of negatives\n",
      "org = 1.69% of negatives\n",
      "blank = 31.99% of negatives\n",
      "email = 0.95% of negatives\n",
      "url = 6.39% of negatives\n",
      "phone = 8.63% of negatives\n"
     ]
    }
   ],
   "source": [
    "ln = len(negative)\n",
    "c = Counter(negative.named_entity)\n",
    "for ent in entity_order:\n",
    "    print(f\"{ent} = {(c[ent]/ln)*100:.2f}% of negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_entity = 44.65% of signaturess\n",
      "person = 13.27% of signaturess\n",
      "org = 9.42% of signaturess\n",
      "blank = 3.99% of signaturess\n",
      "email = 5.14% of signaturess\n",
      "url = 3.71% of signaturess\n",
      "phone = 19.83% of signaturess\n"
     ]
    }
   ],
   "source": [
    "ln = len(signatures)\n",
    "c = Counter(signatures.named_entity)\n",
    "for ent in entity_order:\n",
    "    print(f\"{ent} = {(c[ent]/ln)*100:.2f}% of signaturess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.to_pickle(\"enron_random_clean1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
